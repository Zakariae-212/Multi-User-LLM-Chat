import { NextRequest, NextResponse } from "next/server";
import { ChatRequest, ChatResponse, Message } from "@/app/types/chat";

// üîπ Historique global (in-memory)
let conversationHistory: Message[] = [];

const MAX_HISTORY = 15;

/**
 * API Route pour g√©rer les requ√™tes de chat
 * POST /api/chat
 */
export async function POST(request: NextRequest) {
  try {
    // 1. Lire la requ√™te
    const body: ChatRequest = await request.json();
    const { messages, users, reset } = body;

// üîπ SI NOUVELLE CONVERSATION ‚Üí RESET SERVEUR
    if (reset) {
      conversationHistory = [];
      return NextResponse.json({ success: true });
    }

    if (!messages || !users) {
      return NextResponse.json(
        { error: "Messages et users sont requis" },
        { status: 400 }
      );
    }

    // 2. V√©rifier la cl√© API
    const apiKey = process.env.GOOGLE_API_KEY;
    if (!apiKey) {
      return NextResponse.json(
        { error: "Cl√© API non configur√©e" },
        { status: 500 }
      );
    }

    // 3. Mettre √† jour l‚Äôhistorique serveur (max 15)
    conversationHistory = [...conversationHistory, ...messages].slice(
      -MAX_HISTORY
    );

    // 4. Construire le prompt
    const prompt = buildPrompt(conversationHistory, users);

    // 5. Appel √† Gemini
    const response = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${apiKey}`,
      {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          contents: [
            {
              parts: [{ text: prompt }],
            },
          ],
        }),
      }
    );

    if (!response.ok) {
      const error = await response.text();
      console.error("Erreur Gemini:", error);
      return NextResponse.json(
        { error: "Erreur lors de l'appel √† Gemini" },
        { status: 500 }
      );
    }

    // 6. Extraire la r√©ponse IA
    const data = await response.json();
    const aiText =
      data.candidates?.[0]?.content?.parts?.[0]?.text ||
      "D√©sol√©, je ne peux pas r√©pondre.";

    // 7. Cr√©er le message IA
    const aiMessage: Message = {
      id: `msg-${Date.now()}`,
      userId: "ai",
      content: aiText.trim(),
      timestamp: Date.now(),
    };

    // 8. Ajouter le message IA √† l‚Äôhistorique
    conversationHistory = [...conversationHistory, aiMessage].slice(
      -MAX_HISTORY
    );

    const responseData: ChatResponse = {
      message: aiMessage,
    };

    return NextResponse.json(responseData);
  } catch (error) {
    console.error("Erreur API:", error);
    return NextResponse.json(
      { error: "Erreur serveur" },
      { status: 500 }
    );
  }
}

/**
 * Construction du prompt LLM
 */
function buildPrompt(messages: Message[], users: any[]): string {
  const userMap = new Map(users.map((u) => [u.id, u.name]));

  const usersList = users
    .filter((u) => u.role === "user")
    .map((u) => u.name)
    .join(", ");

  const history = messages
    .map((msg) => {
      const userName = userMap.get(msg.userId) || "Inconnu";
      return `${userName}: ${msg.content}`;
    })
    .join("\n");

  return `
Tu es un assistant IA participant √† une conversation de groupe.

Participants : ${usersList}

R√®gles :
- R√©ponds naturellement comme dans un chat de groupe
- Participer de mani√®re naturelle comme un vrai humain.
- Adresse les utilisateurs par leur nom si pertinent
- Tiens compte du contexte global
- Ne jamais inventer des informations qui n'ont pas √©t√© donn√©es.
- R√©ponds en fran√ßais
- Sois concis

Historique :
${history}

Assistant :
`.trim();
}
